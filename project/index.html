<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">

**Final Project Report**

Student Name: Tao Zhang

Student Number: 23-955-511

nethz: zhangta

# Motivational Image - Newton's Cradle

<img src="./motive.png" alt="Motivational image" class="img-responsive">

<small>
    _By Stable Diffusion, with prompt:
    "A 4-balls Newton's Cradle with the four balls (with different materials) being earth,
    moon, sun and jupyter. The background is the galaxy. In painting style."_
</small>

# Features Overview
ID      | Short Name                |Points | Features
--------|---------------------------|-------|----------------------------------
5.1     | Advanced Camera Effects   | 5  | Depth of Field
5.3     | Images as Textures        | 5  |
5.7     | Intel's Open Image Denoise integration        | 5  |
5.8     | Bump Mapping              | 5  |
5.11    | Textured Area Emitters    | 5  |
5.20    | Modeling Meshes           | 5  |
15.3    | Environment Map Emitter   | 15 |
15.5    | Disney BSDF               | 15 | Subsurface, Roughness, Metallic, Specular, Clearcoat
Total   |                           | 60 |

# Image As Texture (5pts)

`image_texture.cpp` inplements a child class of `Texture` which requires a image input, the importing is using
`stb_image.h` library as suggested in this [tutorial](https://learnopengl.com/Getting-started/Textures). The evaluation mapping from
uv coordinates to image color is introduced in this [blog](https://viclw17.github.io/2019/04/12/raytracing-uv-mapping-and-texturing).
Before returning the color, we need to do the inverse gamma correction to the color.

Additionally, for those DSDFs that I want to use image texture, I need to add texture for the albedo in `addChild()`.

**Comparison: Nori vs. Mitsuba**
<div class="twentytwenty-container">
    <img src="texmap\ImgAsTex_nori.png" alt="nori" class="img-responsive">
    <img src="texmap\ImgAsTex_mitsu.png" alt="mitsuba" class="img-responsive">
</div>

# Bump Mapping (5pts)
`normal_texture.cpp` inplements almost the same as `image_texture.cpp` except that we dont need to do the inverse gamma correction, and
normalize each color channel $c$ by $c = 2c-1$ as suggested in this [tutorial](https://learnopengl.com/Advanced-Lighting/Normal-Mapping).

For those DSDFs that I want to use image texture, I need to add texture for the normal in `addChild()` and add a `get_Normal()` method to
return `m_normal` stored. In `shape.h` I add a function `setNormal()` which calls `getNormal()` and set the new normal to the shadow frame.
And we need to call `setNormal()` whenever a shape calls `setHitInformation()`.

**Comparison: Effect of both (image & bump) texture mappings**
<div class="twentytwenty-container">
    <img src="bumpmap\Bump_plain.png" alt="plain" class="img-responsive">
    <img src="bumpmap\Bump_normal.png" alt="bump only" class="img-responsive">
    <img src="bumpmap\Bump_img.png" alt="img only" class="img-responsive">
    <img src="bumpmap\Bump_both.png" alt="both " class="img-responsive">
</div>

# Textured Area Emitter (5pts)
This new class of emitter is implemented in `arealight_texture.cpp`, which is similar to simple area emitter, except that it
requires to set the uv value when doing the sampling.

So for both `EmitterQueryRecord` and `ShapeQueryRecord` we need to add a
`uv` member, and when we doing path tracing we need to set the uv value for those `EmitterQueryRecord`.
Additionally, when a shape calls `sampleSurface()`, now we also need to set the uv value for the `ShapeQueryRecord`.


**Comparison: Nori vs. Mitsuba**
<div class="twentytwenty-container">
    <img src="texemitter\texEmitter_nori.png" alt="nori" class="img-responsive">
    <img src="texemitter\texEmitter_mitsu.png" alt="mitsuba" class="img-responsive">
</div>

# Part 4: Disney BRDF (15pts)
The implementation is in `disney.cpp` which is heavily based on the original author's [code](https://schuttejoe.github.io/post/disneybsdf/).
I had a very long and hard time to understand all the concepts behind the Disney BRDF, and I am still not sure if I fully understand it. Two
more main references I used are the original [paper](https://media.disneyanimation.com/uploads/production/publication_asset/48/asset/s2012_pbs_disney_brdf_notes_v3.pdf) and
one of UCSD's [homework](https://cseweb.ucsd.edu/~tzli/cse272/wi2023/homework1.pdf), but they are still fairly inconsistent with each other.

My implementation still use the anisotropic version GTR2 and GGX functions, but I just pass the same value to both $\alpha_x$ and $\alpha_y$ to
make it isotropic. The GTR2_aniso function I used for sampling is derived in the supplementary B.2 section from the original [paper](https://media.disneyanimation.com/uploads/production/publication_asset/48/asset/s2012_pbs_disney_brdf_notes_v3.pdf).
While the sample procedure is the same as microfacet BRDF which uses a value `kd` indicating the probability of sampling diffuse reflection.

I originally chose `kd = (1 - m_metallic) * (1 - m_specular) - 0.25 * m_clearcoat`, but this gives me many fireflies. So I changed it to
simply `kd = (1 - m_metallic) / 2.f - 0.25 * m_clearcoat` which gives me much better results.

**Comparison: Specular**
<div class="twentytwenty-container">
    <img src="disney\s_0.png" alt="s=0" class="img-responsive">
    <img src="disney\s_33.png" alt="s=1/3" class="img-responsive">
    <img src="disney\s_100.png" alt="s=1" class="img-responsive">
    <img src="disney\s_66.png" alt="s=2/3" class="img-responsive">
</div>

**Comparison: Roughness**
<div class="twentytwenty-container">
    <img src="disney\r_0.png" alt="r=0" class="img-responsive">
    <img src="disney\r_33.png" alt="r=1/3" class="img-responsive">
    <img src="disney\r_100.png" alt="r=1" class="img-responsive">
    <img src="disney\r_66.png" alt="r=2/3" class="img-responsive">
</div>

**Comparison: Subsurface**
<div class="twentytwenty-container">
    <img src="disney\ss_0.png" alt="ss=0" class="img-responsive">
    <img src="disney\ss_33.png" alt="ss=1/3" class="img-responsive">
    <img src="disney\ss_100.png" alt="ss=1" class="img-responsive">
    <img src="disney\ss_66.png" alt="ss=2/3" class="img-responsive">
</div>

**Comparison: Clearcoat**
<div class="twentytwenty-container">
    <img src="disney\c_0.png" alt="c=0" class="img-responsive">
    <img src="disney\c_33.png" alt="c=1/3" class="img-responsive">
    <img src="disney\c_100.png" alt="c=1" class="img-responsive">
    <img src="disney\c_66.png" alt="c=2/3" class="img-responsive">
</div>

**Comparison: Metallic**
<div class="twentytwenty-container">
    <img src="disney\m_0.png" alt="m=0" class="img-responsive">
    <img src="disney\m_33.png" alt="m=1/3" class="img-responsive">
    <img src="disney\m_100.png" alt="m=1" class="img-responsive">
    <img src="disney\m_66.png" alt="m=2/3" class="img-responsive">
</div>

**Comparison: Nori vs. Mitsuba (m=.3, s=.1, r=0, ss=0, c=.3)**
<div class="twentytwenty-container">
    <img src="disney\bunny_nori.png" alt="nori" class="img-responsive">
    <img src="disney\bunny_mitsu.png" alt="mitsu" class="img-responsive">
</div>

**Warping Visualization & Chi Test for GTR2:**
<p style="text-align:center;"><img src="disney\GTR2.png" alt="GTR2" /></p>
<p style="text-align:center;"><img src="disney\GTR2_chi.png" alt="GTR2 Chi-Test" /></p>

**Warping Visualization & Chi Test for Disney BRDF (m=.5, s=.5, r=.1, ss=.5, c=.5):**
<p style="text-align:center;"><img src="disney\disney.png" alt="Disney BRDF (m=.5, s=.5, r=.1, ss=.5, c=.5)" /></p>
<p style="text-align:center;"><img src="disney\disney_chi.png" alt="Disney BRDF Chi-Test" /></p>


# Environment Map Emitter (15pts)
The implementation is in `environment.cpp` which is done by very straightforwardly following the original
[paper](https://web.cs.wpi.edu/~emmanuel/courses/cs563/S07/projects/envsample.pdf). The only thing we need to
add (which is also done in the Disney BRDF) is to estimate the luminance $l$ based on the RGB color, given by $l = 0.3r + 0.6g + 0.1b$.

Additinally, I found that often the environment map is too bright, so I use a `m_strength` parameter to scale the radiance when
the `eval()` method returns.

**Comparison: Nori vs. Mitsuba**
<div class="twentytwenty-container">
    <img src="env\EnvMap_nori.png" alt="nori" class="img-responsive">
    <img src="env\EnvMap_mitsu.png" alt="mitsu" class="img-responsive">
</div>

# Depth of Field (5pts)
This [tutorial](https://courses.cs.washington.edu/courses/csep557/99au/projects/trace/depthoffield.doc) from UWashington gives a
very clear intuition for me. The implementation is in `dof_camera.cpp`, different from the perspective camera, we have 2 additional
members `m_focalLength` and `m_apertureRadius`. The `sampleRay()` method is also different, we first sample a point on the aperture
and compute the focal point and the direction of secondary ray, and we update the `ray` with the new direction and origin.

**Comparison: Different Aperture Radius**
<div class="twentytwenty-container">
    <img src="dof\nori_dof_0.0.png" alt="r=0" class="img-responsive">
    <img src="dof\nori_dof_0.1.png" alt="r=0.1" class="img-responsive">
    <img src="dof\nori_dof_0.3.png" alt="r=0.3" class="img-responsive">
    <img src="dof\nori_dof_0.2.png" alt="r=0.2" class="img-responsive">
</div>

# Build My Own Meshes (5pts)
I am using Blender to build my own meshes, namely the frame and the bottom of my Newton's Cradle.
I mainly refer to this [tutorial](https://www.youtube.com/watch?v=EsufneMOvWA&t=1415s) to learn how to use Blender.

One important thing I found only when I was doing the final rendering is that the number of vertices of a bevel or a cylinder is
very important, I need to increase it to a considerable number to make it look smooth, because otherwise it does not have enough
geometry information.

**Bottom of Newton's Cradle:**
<p style="text-align:center;"><img src="blender\bottom.png" alt="bottom" /></p>

**Frame of Newton's Cradle:**
<p style="text-align:center;"><img src="blender\frame.png" alt="frame" /></p>

# Intel's Open Image Denoiser (5pts)
For this, I simply download the pre-built binaries from their official [website](https://www.openimagedenoise.org/index.html).

But to actually use it, I have to use another tool called [imagemagick](https://imagemagick.org/) to change the format of the image
and the endianness of the image. The commands I used are:
```
magick convert final.png -endian lsb final.pfm
.\oidnDenoise.exe --hdr final.pfm -o final_de.pfm
magick convert final_de.pfm final_de.png
```

And to make sure the denoiser actually works, I need to change the reconstruction filter to box filter before rendering. And the
denoising result is quite good.

**Comparison:**
<div class="twentytwenty-container">
    <img src="denoise\cgltri.png" alt="original" class="img-responsive">
    <img src="denoise\cgltri_denoised.png" alt="denoised" class="img-responsive">
</div>

# Final Rendering for my Newton's Cradle
<p style="text-align:center;"><img src="denoise\final_de.png" alt="final" /></p>

For the balls I use the celestial body textures from [here](https://www.solarsystemscope.com/textures/), and to make them visible
in low light condition I make them emissive, and to make their surface more interesting I use different Disney BRDF for them. The frame is made
of metal thanks to metallic parameter in Disney BRDF, and the bottom is made of wood thanks to the wonderful texture from
[here](https://polyhaven.com/). The surface is also an textured area emitter, which is made of milky way. The background
if a starry night sky presented by the environment map emitter. Finally, I am looking at Jupiter using a depth of field camera.



<!-- Bootstrap core CSS and JavaScript -->

<link href="../resources/offcanvas.css" rel="stylesheet">
<link href="../resources/twentytwenty.css" rel="stylesheet" type="text/css" />

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="../resources/bootstrap.min.js"></script>
<script src="../resources/jquery.event.move.js"></script>
<script src="../resources/jquery.twentytwenty.js"></script>

<script>
    $(window).load(function () { $(".twentytwenty-container").twentytwenty({ default_offset_pct: 0.5 }); });
</script>

<!-- Markdeep: -->
<script>var markdeepOptions = { onLoad: function () { $(".twentytwenty-container").twentytwenty({ default_offset_pct: 0.5, move_slider_on_hover: true }); }, tocStyle: 'none' };</script>
<script src="https://morgan3d.github.io/markdeep/latest/markdeep.min.js?" charset="utf-8"></script>
<script>window.alreadyProcessedMarkdeep || (document.body.style.visibility = "visible")</script>
